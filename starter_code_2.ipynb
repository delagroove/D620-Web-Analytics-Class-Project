{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Machine Learning Classifers with NLTK\n",
    "\n",
    "## CUNY DATA 620 Web Analytics\n",
    "\n",
    "### Fall 2018 Semester Class Project\n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "- Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. \n",
    "\n",
    "\n",
    "- Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. \n",
    "\n",
    "\n",
    "- Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. \n",
    "\n",
    "\n",
    "- How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect? \n",
    "\n",
    "Source: Natural Language Processing with Python, exercise 6.10.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Using the NLTK's `names` corpus, we will be demonstrating how to create and improve supervised classification models. The corpus contains 2 files called `female.txt` and `male.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "names.fileids() # confirm male and female txt files exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the male & female names, look at the first 20 entries for each and store them together in a variable called `people`. We will normalize the names by changing them to lowercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aamir', 'aaron', 'abbey', 'abbie', 'abbot', 'abbott', 'abby', 'abdel', 'abdul', 'abdulkarim', 'abdullah', 'abe', 'abel', 'abelard', 'abner', 'abraham', 'abram', 'ace', 'adair', 'adam']\n",
      "['abagael', 'abagail', 'abbe', 'abbey', 'abbi', 'abbie', 'abby', 'abigael', 'abigail', 'abigale', 'abra', 'acacia', 'ada', 'adah', 'adaline', 'adara', 'addie', 'addis', 'adel', 'adela']\n"
     ]
    }
   ],
   "source": [
    "# load male and female  name files from nltk.names; store in people list\n",
    "males, females = [n.lower() for n in names.words('male.txt')], [n.lower() for n in names.words('female.txt')] \n",
    "\n",
    "print([male for male in males[:20]])\n",
    "print([female for female in females[:20]])\n",
    "\n",
    "people = males + females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7944 names in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_people = len(people)\n",
    "n_people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the variable `gender`, which will contain the gender labels for each of the names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make gender list\n",
    "gender = list(repeat('male', len(males))) + list(repeat('female', len(females)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the frequency of the male and female names, we see that there is a class imbalance between the two labels. There are considerably more females than males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22f40632128>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_counts = pd.Series(gender).value_counts()\n",
    "gender_counts.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, more than 60% are female. We will want to be cognizant of this class imbalance when we choose the metrics to evaluate the strength of our model. If we simply predicted that every name was female, we would be correct more than 60% of the time. We may also want to consider stratifying the training, development and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22f41142518>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_counts.divide(n_people).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Our raw dataset contains only one predictor variable.  This feature indicates the first name of each observed individual in our data.  Unfortunately, the name variable--in its unprocessed form--will not be very useful for building an accurate gender classification model.  A model constructed using first names only will struggle to predict gender for names not explicitly identified in the training data.  Also, our dataset contains a unique collection of first names within each gender class, but has a small number of overlapping names between gender types.  Without additional feature engineering, our models will not be able to make reasonable guesses when encountering gender-neutral names.  \n",
    "\n",
    "The textbook, *Natural Language Processing with Python*, provides a number of helpful suggestions for extracting new features from first names for gender classification purposes:\n",
    "* isolate the first letter of each name\n",
    "* isolate the last letter of each name\n",
    "* isolate the last two letters of each name.\n",
    "\n",
    "We decided to include these features as possible predictors for our models.  These extracted features can reveal common patterns in the prefixes and suffixes of first names that are often associated with a particular gender.  For instance, many female first names end with the letter \"a\".  \n",
    "\n",
    "Building on the text's suggestions, we also extracted the following features:\n",
    "* the first two letters of each name\n",
    "* the first three letters of each name\n",
    "* the last three letters of each name\n",
    "\n",
    "There were a handful of additional features that we thought might be relevant for gender identification:\n",
    "* the number of vowels in each name\n",
    "* The first two non-contiguous letters of each name \n",
    "* The first three non-contiguous letters\n",
    "* The last two non-contiguous letters\n",
    "* the last three non-contiguous letters\n",
    "\n",
    "We also found an [article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4446333/) from an academic journal that characterizes certain letters as \"round\" and others as \"sharp\".  The authors contend that round letters tend to be associated with female names, while sharp letters are more often associated with male names.  Using this information, we created the following potential features:\n",
    "* the number of round, consonant letters (\"b\",\"m\", \"l\", and \"n\") in each name\n",
    "* the number of sharp, consant letters (\"k\", \"p\", and \"t\") per name\n",
    "* the number of round vowels (\"u\" and \"o\") in each first name.\n",
    "\n",
    "Finally, we found a Wordpress [blog entry](https://debuk.wordpress.com/tag/feminine-suffixes/) that discusses common female suffixes in first names.  These include names ending in ending in \"a\", \"y\", \"ie\", and \"ah\".  We used this information to create a new binary variable that identifies if one of these suffixes is present in a given name.\n",
    "\n",
    "In the script below, we create a function, *gender_features()*, that returns a dictionary of the extracted features described in this section.  The function has multiple arguments:\n",
    "* a first name to use for extracting features\n",
    "* a list of desired features.  This argument can be used to vary the features that are returned by the function \n",
    "\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce features\n",
    "def gender_features(word, *args):\n",
    "    \"\"\"\n",
    "    function returns dictionary of features\n",
    "        word: name to extract features from\n",
    "        args:  one or more strings to specify desired features, including:\n",
    "                'length','first','first2','first3', 'last', 'last2', 'last3',\n",
    "                'every_other2_beg','every_other3_beg', 'every_other2_end', 'every_other3_end',\n",
    "                'vowel_ct', 'round_cons_ct', 'sharp_cons_ct','round_vowel_ct',\n",
    "                trad_female_end'\n",
    "               \n",
    "    \"\"\"\n",
    "    \n",
    "    gf = {}\n",
    "    \n",
    "    # word length\n",
    "    gf['length'] = len(word)\n",
    "   \n",
    "    # first letters\n",
    "    gf['first'] = word[0].lower()\n",
    "    gf['first2'] = word[0:2].lower()\n",
    "    gf['first3'] = word[0:3].lower() if gf['length'] >2  else word[0:2].lower()\n",
    "    \n",
    "    gf['two_letters'] = 'y' if len(word) == 2 else 'n'\n",
    "    \n",
    "    # last letters\n",
    "    gf['last'] = word[-1].lower()\n",
    "    gf['last2'] = word[-2:].lower()\n",
    "    gf['last3'] = word[-3:].lower() if gf['length'] >2  else word[-2:].lower()\n",
    "    \n",
    "    # every other beg\n",
    "    gf['every_other2_beg'] = word[0]+word[2] if gf['length'] > 2 else word[0]\n",
    "    gf['every_other3_beg'] = gf['every_other2_beg']+word[4]  if gf['length'] > 4 else \\\n",
    "    gf['every_other2_beg']\n",
    "    \n",
    "    # every other end\n",
    "    gf['every_other2_end'] = word[-3]+word[-1] if gf['length'] > 2 else word[-1]\n",
    "    gf['every_other3_end'] = word[-5]+gf['every_other2_end']  if gf['length'] > 4 else \\\n",
    "    gf['every_other2_end']\n",
    "    \n",
    "    # count: vowels, rounded consonants, sharp consonants\n",
    "    for letter in word:\n",
    "        # count vowels\n",
    "        if letter in 'aeiou':\n",
    "            gf['vowel_ct'] = gf.get('vowel_ct',0) + 1\n",
    "        # count rounded consonants\n",
    "        if letter in 'bmln':\n",
    "            gf['round_cons_ct'] = gf.get('round_cons_ct',0) + 1\n",
    "        # count sharp consonants\n",
    "        if letter in 'k,p,t':\n",
    "            gf['sharp_cons_ct'] = gf.get('sharp_cons_ct',0) + 1\n",
    "        # count rounded vowels\n",
    "        if letter in 'uo':\n",
    "            gf['round_vowel_ct'] = gf.get('round_vowel_ct',0) + 1\n",
    "            \n",
    "    # traditional feminine ending, 'y' or 'n'\n",
    "    gf['trad_female_end'] = 'y' if gf['last2'] in ['ie','ah'] or \\\n",
    "    gf['last'] in ['a','y'] else 'n'\n",
    "    \n",
    "    ## patterns: double consonant ends in y: Binny, Daffy...\n",
    "    #gf['consonant_y'] = 'y' if bool(re.search(r\"([b-df-hj-np-tv-z])\\1{1,}y$\", word)) else 'n'\n",
    "    \n",
    "    # generate dictionary subset\n",
    "    return(dict((k, gf[k]) for k in args if k in gf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length': 5,\n",
       " 'first': 'b',\n",
       " 'first2': 'bi',\n",
       " 'first3': 'bin',\n",
       " 'last': 'y',\n",
       " 'last2': 'ny',\n",
       " 'last3': 'nny',\n",
       " 'every_other2_beg': 'Bn',\n",
       " 'every_other3_beg': 'Bny',\n",
       " 'every_other2_end': 'ny',\n",
       " 'every_other3_end': 'Bny',\n",
       " 'vowel_ct': 1,\n",
       " 'round_cons_ct': 2,\n",
       " 'trad_female_end': 'y'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify which features to use\n",
    "myargs = ['length','first','first2','first3', 'last', 'last2', 'last3', \\\n",
    "          'every_other2_beg','every_other3_beg', 'every_other2_end', 'every_other3_end', \\\n",
    "          'vowel_ct', 'round_cons_ct', 'sharp_cons_ct','round_vowel_ct', \\\n",
    "          'trad_female_end']\n",
    "\n",
    "# specify name, and argument list \n",
    "gender_features('Binny', *myargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Development & Test Sets\n",
    "\n",
    "Let's partition our names & labels into 3 buckets. We will stratify each bucket in order to maintain our class imbalance and set the random state to 4.\n",
    "\n",
    "- First, let's set aside 500 observations that will not be used in training or development. We will use this test set to evaluate the strength of the model.\n",
    "\n",
    "- Then, let's hold out another 500 observations to use as a development set to do error analysis.\n",
    "\n",
    "- Finally, we will use the remaining 6944 names to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set aside 500 samples for testing\n",
    "people_dev_train, people_test, gender_dev_train, gender_test = train_test_split(people, \n",
    "                                                                        gender, \n",
    "                                                                        test_size=500, \n",
    "                                                                        stratify=gender, \n",
    "                                                                        random_state=4)\n",
    "\n",
    "# set aside 500 samples for development\n",
    "people_train, people_devtest, gender_train, gender_devtest = train_test_split(people_dev_train, \n",
    "                                                                            gender_dev_train, \n",
    "                                                                            test_size=500, \n",
    "                                                                            stratify=gender_dev_train,\n",
    "                                                                            random_state=4)\n",
    "\n",
    "# list of tuples (name, gender)\n",
    "train_names = list(zip(people_train, gender_train))\n",
    "devtest_names = list(zip(people_devtest, gender_devtest))\n",
    "test_names = list(zip(people_test, gender_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "Let's use the naive Bayes (NB) classifier to build our first model. This model derives its \"naiviete\" from its independence asssumption. NB assumes that all features are independent of one another, but features within real-world data often contain dependencies. \n",
    "\n",
    "## NB Round 1\n",
    "\n",
    "Since using too many dependent features can cause the NB model to double count the effect of what is essentially the same feature, let's initiate the model with only a handful of features and try the additive approach. We will use the name length, first character and last character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(names, genders, features):\n",
    "    \"\"\"\n",
    "    function returns list of tuples (name features, gender)\n",
    "        names: list of names to extract features from\n",
    "        genders: list of gender values\n",
    "        features: list of feature functions           \n",
    "    \"\"\"    \n",
    "    return list(zip(map(lambda d: gender_features(d, *features), names), genders))\n",
    "\n",
    "initial_NB_features = ['length', 'last', 'first']\n",
    "\n",
    "train_features  = create_features(people_train, gender_train, initial_NB_features)\n",
    "devtest_features = create_features(people_devtest, gender_devtest, initial_NB_features)\n",
    "test_features = create_features(people_test, gender_test, initial_NB_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy between the training and development sets is decent at .78."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n"
     ]
    }
   ],
   "source": [
    "# train naive bayes classifier \n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "# classifer accuracy on validation set\n",
    "print(nltk.classify.accuracy(nb_classifier, devtest_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the likelihood ratios of most informative features so far. The last character provides the most information. We should probably try adding more features from the end of the name to see if they improve the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    last = 'a'            female : male   =     34.9 : 1.0\n",
      "                    last = 'k'              male : female =     30.3 : 1.0\n",
      "                    last = 'f'              male : female =     15.3 : 1.0\n",
      "                    last = 'p'              male : female =     11.2 : 1.0\n",
      "                    last = 'v'              male : female =     11.2 : 1.0\n",
      "                    last = 'd'              male : female =     10.1 : 1.0\n",
      "                    last = 'o'              male : female =      8.3 : 1.0\n",
      "                    last = 'm'              male : female =      8.2 : 1.0\n",
      "                    last = 'r'              male : female =      6.8 : 1.0\n",
      "                    last = 'z'              male : female =      6.4 : 1.0\n",
      "                    last = 'b'              male : female =      6.3 : 1.0\n",
      "                    last = 'w'              male : female =      5.1 : 1.0\n",
      "                    last = 'g'              male : female =      4.9 : 1.0\n",
      "                   first = 'w'              male : female =      4.5 : 1.0\n",
      "                    last = 's'              male : female =      4.2 : 1.0\n",
      "                    last = 't'              male : female =      4.0 : 1.0\n",
      "                    last = 'i'            female : male   =      3.7 : 1.0\n",
      "                   first = 'q'              male : female =      3.1 : 1.0\n",
      "                    last = 'u'              male : female =      2.7 : 1.0\n",
      "                  length = 2                male : female =      2.4 : 1.0\n",
      "                   first = 'k'            female : male   =      2.2 : 1.0\n",
      "                    last = 'x'              male : female =      2.1 : 1.0\n",
      "                   first = 'h'              male : female =      2.1 : 1.0\n",
      "                    last = 'n'              male : female =      2.1 : 1.0\n",
      "                  length = 3                male : female =      2.1 : 1.0\n",
      "                   first = 'u'              male : female =      2.0 : 1.0\n",
      "                    last = 'e'            female : male   =      1.8 : 1.0\n",
      "                    last = 'l'              male : female =      1.7 : 1.0\n",
      "                  length = 15               male : female =      1.7 : 1.0\n",
      "                   first = 'x'              male : female =      1.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# look at most informative features\n",
    "nb_classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at some of the model's misclassified names. From looking at the errors, we see that using only the first and last character is too simplistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual, guess, name: \n",
      "\n",
      "('male', 'female', 'wye')\n",
      "('male', 'female', 'llewellyn')\n",
      "('male', 'female', 'smith')\n",
      "('male', 'female', 'garry')\n",
      "('male', 'female', 'thadeus')\n",
      "('male', 'female', 'jereme')\n",
      "('male', 'female', 'harvie')\n",
      "('male', 'female', 'grant')\n",
      "('male', 'female', 'gerrit')\n",
      "('male', 'female', 'woodman')\n",
      "('male', 'female', 'owen')\n",
      "('male', 'female', 'roderic')\n",
      "('male', 'female', 'moses')\n",
      "('male', 'female', 'arvin')\n",
      "('male', 'female', 'moishe')\n",
      "('male', 'female', 'georges')\n",
      "('male', 'female', 'redmond')\n",
      "('male', 'female', 'jud')\n",
      "('male', 'female', 'davin')\n",
      "('male', 'female', 'abe')\n",
      "('male', 'female', 'whitaker')\n",
      "('male', 'female', 'sheffy')\n",
      "('male', 'female', 'michale')\n",
      "('male', 'female', 'rufus')\n",
      "('male', 'female', 'bary')\n",
      "('male', 'female', 'paten')\n",
      "('male', 'female', 'jehu')\n",
      "('male', 'female', 'martyn')\n",
      "('male', 'female', 'bartolemo')\n",
      "('male', 'female', 'trace')\n"
     ]
    }
   ],
   "source": [
    "# look at names that were mis-classified\n",
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    #print(name)\n",
    "    guess = nb_classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, name))\n",
    "\n",
    "print('actual, guess, name: \\n')\n",
    "for x in errors[:30]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Round 2\n",
    "\n",
    "Since we did some error analysis, let's make sure that we reset our training and development set, so that the model doesn't become biased towards the previous round's idiosyncracies. We will use a different random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "people_train_v2, people_devtest_v2, gender_train_v2, gender_devtest_v2 = train_test_split(people_dev_train, \n",
    "                                                                            gender_dev_train, \n",
    "                                                                            test_size=500, \n",
    "                                                                            stratify=gender_dev_train,\n",
    "                                                                            random_state=5)\n",
    "\n",
    "# list of tuples (name, gender)\n",
    "train_names_v2 = list(zip(people_train_v2, gender_train_v2))\n",
    "devtest_names_v2 = list(zip(people_devtest_v2, gender_devtest_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this 2nd round, let's add some new features from the end characters of the names. We will add the last 2 and the last 3 letters. We will also add the new features to the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_features_v2 = initial_NB_features + ['last2', 'last3']\n",
    "\n",
    "train_features_v2  = create_features(people_train_v2, gender_train_v2, NB_features_v2)\n",
    "devtest_features_v2 = create_features(people_devtest_v2, gender_devtest_v2, NB_features_v2)\n",
    "test_features_v2 = create_features(people_test, gender_test, NB_features_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the model again using the new features. The accuracy score has improved from .78 to .788."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788\n"
     ]
    }
   ],
   "source": [
    "# train naive bayes classifier \n",
    "nb_classifier_v2 = nltk.NaiveBayesClassifier.train(train_features_v2)\n",
    "# # classifer accuracy on validation set\n",
    "print(nltk.classify.accuracy(nb_classifier_v2, devtest_features_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the 1st round features, the `last2` feature adds a lot of information to the model and `last3` as well to a lesser degree. When the last 2 letters are 'na', the name is 94 times more often female than male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   last2 = 'na'           female : male   =     94.1 : 1.0\n",
      "                   last2 = 'ia'           female : male   =     38.5 : 1.0\n",
      "                   last2 = 'ld'             male : female =     37.2 : 1.0\n",
      "                   last2 = 'us'             male : female =     35.6 : 1.0\n",
      "                    last = 'a'            female : male   =     33.6 : 1.0\n",
      "                   last2 = 'sa'           female : male   =     32.6 : 1.0\n",
      "                    last = 'k'              male : female =     30.8 : 1.0\n",
      "                   last3 = 'nne'          female : male   =     29.8 : 1.0\n",
      "                   last2 = 'rd'             male : female =     25.3 : 1.0\n",
      "                   last2 = 'do'             male : female =     25.0 : 1.0\n",
      "                   last2 = 'ta'           female : male   =     24.8 : 1.0\n",
      "                   last3 = 'ana'          female : male   =     24.2 : 1.0\n",
      "                   last3 = 'tta'          female : male   =     24.2 : 1.0\n",
      "                   last2 = 'io'             male : female =     23.9 : 1.0\n",
      "                   last2 = 'ra'           female : male   =     22.4 : 1.0\n",
      "                   last3 = 'ard'            male : female =     21.6 : 1.0\n",
      "                   last3 = 'vin'            male : female =     20.2 : 1.0\n",
      "                   last2 = 'rt'             male : female =     19.7 : 1.0\n",
      "                   last2 = 'os'             male : female =     17.2 : 1.0\n",
      "                   last3 = 'ita'          female : male   =     16.1 : 1.0\n",
      "                    last = 'f'              male : female =     15.9 : 1.0\n",
      "                   last3 = 'son'            male : female =     15.3 : 1.0\n",
      "                   last3 = 'old'            male : female =     15.0 : 1.0\n",
      "                   last2 = 'im'             male : female =     15.0 : 1.0\n",
      "                   last3 = 'ria'          female : male   =     14.4 : 1.0\n",
      "                   last3 = 'dra'          female : male   =     12.6 : 1.0\n",
      "                    last = 'p'              male : female =     12.5 : 1.0\n",
      "                   last2 = 'ch'             male : female =     11.7 : 1.0\n",
      "                   last2 = 'ns'             male : female =     11.7 : 1.0\n",
      "                    last = 'v'              male : female =     11.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# # look at most informative features\n",
    "nb_classifier_v2.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Testing\n",
    "\n",
    "Let's evaluate our model from round #2 against the test set that we have been holding out from our feature selection process. At 0.816, the features and model perform well against this new set of data. The accuracy is actually a little higher than the model's accuracy against the development set. If model's accuracy decreased vis-a-vis the test set, we would be concerned that the model was overfit to our training data, but that does not appear to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(nb_classifier_v2, test_features_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy Classification\n",
    "We'll use Maximum Entropy Classification to try and get the genders as well.Unlike the Naive Bayes classifier that was used before, this classifier does not assume that the features are conditionally independent of each other. As it's name states, the Maximum Entropy classifier looks at the models that fit our training dataselects the one which has the largest entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the entropy of the labels in our dataset. The higher the entropy the better our classification algorithm. The function below shows that we have entropy at 95% which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951030970454714\n"
     ]
    }
   ],
   "source": [
    "def entropy(labels):    \n",
    "    freqdist = nltk.FreqDist(labels)    \n",
    "    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]    \n",
    "    return -sum([p * math.log(p,2) for p in probs])\n",
    "\n",
    "print (entropy(gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make our training, development, and test sets and them build a maximum entropy classifier based on the features. By default it will run for 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.43217        0.820\n",
      "             3          -0.35012        0.873\n",
      "             4          -0.30505        0.886\n",
      "             5          -0.27612        0.896\n",
      "             6          -0.25556        0.902\n",
      "             7          -0.23990        0.909\n",
      "             8          -0.22741        0.913\n",
      "             9          -0.21708        0.918\n",
      "            10          -0.20833        0.920\n",
      "            11          -0.20077        0.923\n",
      "            12          -0.19413        0.925\n",
      "            13          -0.18823        0.928\n",
      "            14          -0.18294        0.930\n",
      "            15          -0.17815        0.931\n",
      "            16          -0.17378        0.933\n",
      "            17          -0.16978        0.934\n",
      "            18          -0.16610        0.935\n",
      "            19          -0.16269        0.936\n",
      "            20          -0.15952        0.937\n",
      "            21          -0.15656        0.939\n",
      "            22          -0.15380        0.940\n",
      "            23          -0.15121        0.940\n",
      "            24          -0.14877        0.941\n",
      "            25          -0.14647        0.942\n",
      "            26          -0.14430        0.943\n",
      "            27          -0.14224        0.943\n",
      "            28          -0.14029        0.944\n",
      "            29          -0.13843        0.944\n",
      "            30          -0.13667        0.944\n",
      "            31          -0.13498        0.945\n",
      "            32          -0.13338        0.945\n",
      "            33          -0.13184        0.945\n",
      "            34          -0.13037        0.946\n",
      "            35          -0.12896        0.946\n",
      "            36          -0.12761        0.946\n",
      "            37          -0.12631        0.947\n",
      "            38          -0.12506        0.947\n",
      "            39          -0.12386        0.947\n",
      "            40          -0.12271        0.948\n",
      "            41          -0.12159        0.948\n",
      "            42          -0.12051        0.948\n",
      "            43          -0.11948        0.948\n",
      "            44          -0.11847        0.948\n",
      "            45          -0.11750        0.949\n",
      "            46          -0.11656        0.949\n",
      "            47          -0.11565        0.949\n",
      "            48          -0.11477        0.949\n",
      "            49          -0.11392        0.949\n",
      "            50          -0.11309        0.949\n",
      "            51          -0.11229        0.949\n",
      "            52          -0.11150        0.949\n",
      "            53          -0.11075        0.949\n",
      "            54          -0.11001        0.949\n",
      "            55          -0.10929        0.949\n",
      "            56          -0.10859        0.950\n",
      "            57          -0.10791        0.950\n",
      "            58          -0.10725        0.950\n",
      "            59          -0.10661        0.950\n",
      "            60          -0.10598        0.950\n",
      "            61          -0.10536        0.950\n",
      "            62          -0.10477        0.950\n",
      "            63          -0.10418        0.951\n",
      "            64          -0.10361        0.951\n",
      "            65          -0.10306        0.951\n",
      "            66          -0.10251        0.951\n",
      "            67          -0.10198        0.951\n",
      "            68          -0.10146        0.951\n",
      "            69          -0.10096        0.951\n",
      "            70          -0.10046        0.951\n",
      "            71          -0.09998        0.951\n",
      "            72          -0.09950        0.951\n",
      "            73          -0.09904        0.951\n",
      "            74          -0.09858        0.951\n",
      "            75          -0.09813        0.951\n",
      "            76          -0.09770        0.951\n",
      "            77          -0.09727        0.951\n",
      "            78          -0.09685        0.951\n",
      "            79          -0.09644        0.951\n",
      "            80          -0.09603        0.951\n",
      "            81          -0.09564        0.951\n",
      "            82          -0.09525        0.952\n",
      "            83          -0.09487        0.952\n",
      "            84          -0.09449        0.952\n",
      "            85          -0.09413        0.952\n",
      "            86          -0.09377        0.952\n",
      "            87          -0.09341        0.952\n",
      "            88          -0.09307        0.952\n",
      "            89          -0.09272        0.952\n",
      "            90          -0.09239        0.952\n",
      "            91          -0.09206        0.952\n",
      "            92          -0.09173        0.952\n",
      "            93          -0.09141        0.952\n",
      "            94          -0.09110        0.952\n",
      "            95          -0.09079        0.952\n",
      "            96          -0.09049        0.952\n",
      "            97          -0.09019        0.952\n",
      "            98          -0.08990        0.952\n",
      "            99          -0.08961        0.952\n",
      "         Final          -0.08932        0.952\n"
     ]
    }
   ],
   "source": [
    "# list of tuples, gender features, gender\n",
    "train_set = list(zip(map(lambda d: gender_features(d, *myargs), people_train),gender_train))\n",
    "devtest_set = list(zip(map(lambda d: gender_features(d, *myargs), people_devtest),gender_devtest))\n",
    "test_set = list(zip(map(lambda d: gender_features(d, *myargs), people_test),gender_test))\n",
    "\n",
    "\n",
    "# list of tuples, names, gender\n",
    "train_names = list(zip(people_train,gender_train))\n",
    "devtest_names = list(zip(people_devtest,gender_devtest))\n",
    "test_names = list(zip(people_test, gender_test))\n",
    "\n",
    "classifier = nltk.classify.MaxentClassifier.train(\n",
    "                         train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see what our model thinks are the most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6.896 first3=='eti' and label is 'male'\n",
      "   6.808 every_other3_end=='mtr' and label is 'female'\n",
      "   5.695 every_other3_beg=='nkt' and label is 'male'\n",
      "   5.563 every_other3_end=='ylr' and label is 'female'\n",
      "   5.312 every_other3_end=='gyn' and label is 'male'\n",
      "   5.166 every_other3_beg=='sdh' and label is 'male'\n",
      "   5.128 every_other3_end=='ir' and label is 'female'\n",
      "   4.992 every_other3_beg=='le' and label is 'male'\n",
      "   4.958 every_other3_end=='gad' and label is 'female'\n",
      "  -4.928 last2=='us' and label is 'female'\n",
      "   4.895 every_other3_beg=='ban' and label is 'male'\n",
      "   4.885 every_other3_end=='ita' and label is 'male'\n",
      "  -4.878 every_other2_end=='la' and label is 'male'\n",
      "   4.849 last2=='aa' and label is 'male'\n",
      "   4.849 last3=='laa' and label is 'male'\n",
      "   4.799 every_other3_end=='fre' and label is 'female'\n",
      "   4.774 last3=='nly' and label is 'male'\n",
      "   4.755 last3=='rko' and label is 'male'\n",
      "   4.752 every_other3_end=='tin' and label is 'male'\n",
      "   4.752 first3=='rez' and label is 'male'\n",
      "   4.752 last3=='eza' and label is 'male'\n",
      "   4.739 last3=='rve' and label is 'male'\n",
      "   4.357 every_other3_end=='ce' and label is 'male'\n",
      "   4.352 every_other3_end=='iha' and label is 'male'\n",
      "   4.350 every_other3_end=='kee' and label is 'male'\n",
      "  -4.338 first3=='tha' and label is 'female'\n",
      "   4.290 every_other3_end=='im' and label is 'male'\n",
      "  -4.289 last2=='na' and label is 'male'\n",
      "  -4.275 every_other2_end=='ea' and label is 'male'\n",
      "  -4.230 last2=='os' and label is 'female'\n",
      "   4.206 every_other3_beg=='sod' and label is 'female'\n",
      "   4.195 first3=='fal' and label is 'female'\n",
      "  -4.190 last=='k' and label is 'female'\n",
      "   4.134 every_other3_beg=='msh' and label is 'male'\n",
      "   4.134 every_other3_end=='ica' and label is 'male'\n",
      "  -4.126 last=='a' and label is 'male'\n",
      "   4.120 every_other3_end=='bl' and label is 'male'\n",
      "   4.093 every_other3_beg=='pte' and label is 'female'\n",
      "   4.082 last3=='isa' and label is 'male'\n",
      "  -4.081 every_other2_beg=='kt' and label is 'male'\n",
      "  -4.066 last3=='nne' and label is 'male'\n",
      "   4.051 every_other3_end=='tve' and label is 'male'\n",
      "   4.033 last3=='nky' and label is 'male'\n",
      "   3.968 every_other3_beg=='als' and label is 'female'\n",
      "  -3.949 first2=='hu' and label is 'female'\n",
      "  -3.939 every_other2_beg=='nn' and label is 'male'\n",
      "  -3.900 every_other3_end=='hr' and label is 'male'\n",
      "   3.889 last3=='tya' and label is 'male'\n",
      "   3.859 first3=='lyl' and label is 'male'\n",
      "   3.809 first3=='dyl' and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuarcy of our classifer accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what our model considers most informative and see that it's 82% accurate let's take a look at which names it got wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual, guess, name: \n",
      "\n",
      "('female', 'male', 'ninnetta')\n",
      "('female', 'male', 'denys')\n",
      "('female', 'male', 'janean')\n",
      "('female', 'male', 'rianon')\n",
      "('female', 'male', 'dede')\n",
      "('female', 'male', 'michell')\n",
      "('female', 'male', 'catie')\n",
      "('female', 'male', 'amalea')\n",
      "('female', 'male', 'gwenora')\n",
      "('female', 'male', 'rozanne')\n",
      "('female', 'male', 'gunilla')\n",
      "('female', 'male', 'alys')\n",
      "('female', 'male', 'lolita')\n",
      "('female', 'male', 'maureen')\n",
      "('female', 'male', 'florry')\n",
      "('female', 'male', 'tiphani')\n",
      "('female', 'male', 'elysha')\n",
      "('female', 'male', 'ruthy')\n",
      "('female', 'male', 'bride')\n",
      "('female', 'male', 'lindsey')\n",
      "('female', 'male', 'melli')\n",
      "('female', 'male', 'florance')\n",
      "('female', 'male', 'ericha')\n",
      "('female', 'male', 'aprilette')\n",
      "('female', 'male', 'norah')\n",
      "('female', 'male', 'noella')\n",
      "('female', 'male', 'jan')\n",
      "('female', 'male', 'nettle')\n",
      "('female', 'male', 'eustacia')\n",
      "('female', 'male', 'ameline')\n",
      "('female', 'male', 'mina')\n",
      "('female', 'male', 'brita')\n",
      "('female', 'male', 'danelle')\n",
      "('female', 'male', 'alyss')\n",
      "('female', 'male', 'seana')\n",
      "('female', 'male', 'jessy')\n",
      "('female', 'male', 'engracia')\n",
      "('female', 'male', 'esmaria')\n",
      "('female', 'male', 'maurene')\n",
      "('female', 'male', 'genovera')\n",
      "('female', 'male', 'miguelita')\n",
      "('female', 'male', 'barbette')\n",
      "('female', 'male', 'merrilee')\n",
      "('female', 'male', 'hazel')\n",
      "('female', 'male', 'mei')\n",
      "('female', 'male', 'donetta')\n",
      "('female', 'male', 'loraine')\n",
      "('female', 'male', 'bettie')\n",
      "('female', 'male', 'demetria')\n",
      "('female', 'male', 'dorisa')\n",
      "('female', 'male', 'marje')\n",
      "('female', 'male', 'elinor')\n",
      "('female', 'male', 'audie')\n",
      "('female', 'male', 'dorolice')\n",
      "('female', 'male', 'lanette')\n",
      "('female', 'male', 'christal')\n",
      "('female', 'male', 'bella')\n",
      "('female', 'male', 'leonie')\n",
      "('female', 'male', 'josephine')\n",
      "('female', 'male', 'stephine')\n",
      "('female', 'male', 'nyssa')\n",
      "('female', 'male', 'nerita')\n",
      "('female', 'male', 'shari')\n",
      "('female', 'male', 'elyn')\n",
      "('female', 'male', 'rhodia')\n",
      "('female', 'male', 'carolann')\n",
      "('female', 'male', 'larina')\n",
      "('female', 'male', 'shela')\n",
      "('female', 'male', 'jennette')\n",
      "('female', 'male', 'tina')\n",
      "('female', 'male', 'amelita')\n",
      "('female', 'male', 'modestia')\n",
      "('female', 'male', 'julia')\n",
      "('female', 'male', 'zara')\n",
      "('female', 'male', 'martelle')\n",
      "('female', 'male', 'tessa')\n",
      "('female', 'male', 'malena')\n",
      "('female', 'male', 'imogen')\n",
      "('female', 'male', 'sharron')\n",
      "('female', 'male', 'terry')\n",
      "('female', 'male', 'melanie')\n",
      "('female', 'male', 'nike')\n",
      "('female', 'male', 'celisse')\n",
      "('female', 'male', 'gayla')\n",
      "('female', 'male', 'pearla')\n",
      "('female', 'male', 'ailee')\n",
      "('female', 'male', 'atlante')\n",
      "('female', 'male', 'izabel')\n",
      "('female', 'male', 'lucie')\n",
      "('female', 'male', 'annora')\n",
      "('female', 'male', 'jacinda')\n",
      "('female', 'male', 'kristyn')\n",
      "('female', 'male', 'marci')\n",
      "('female', 'male', 'dixie')\n",
      "('female', 'male', 'charlena')\n",
      "('female', 'male', 'austine')\n",
      "('female', 'male', 'luelle')\n",
      "('female', 'male', 'fran')\n",
      "('female', 'male', 'myrle')\n",
      "('female', 'male', 'allie')\n",
      "('female', 'male', 'paige')\n",
      "('female', 'male', 'brandie')\n",
      "('female', 'male', 'olwen')\n",
      "('female', 'male', 'maiga')\n",
      "('female', 'male', 'neda')\n",
      "('female', 'male', 'almeta')\n",
      "('female', 'male', 'cacilie')\n",
      "('female', 'male', 'linnell')\n",
      "('female', 'male', 'emelda')\n",
      "('female', 'male', 'clary')\n",
      "('female', 'male', 'nady')\n",
      "('female', 'male', 'fidelity')\n",
      "('female', 'male', 'tierney')\n",
      "('female', 'male', 'penni')\n",
      "('female', 'male', 'elise')\n",
      "('female', 'male', 'rosana')\n",
      "('female', 'male', 'faustine')\n",
      "('female', 'male', 'vivyan')\n",
      "('female', 'male', 'monah')\n",
      "('female', 'male', 'lorna')\n",
      "('female', 'male', 'dianna')\n",
      "('female', 'male', 'jaine')\n",
      "('female', 'male', 'velma')\n",
      "('female', 'male', 'yolanthe')\n",
      "('female', 'male', 'nancey')\n",
      "('female', 'male', 'phyllys')\n",
      "('female', 'male', 'arlie')\n",
      "('female', 'male', 'blithe')\n",
      "('female', 'male', 'carmella')\n",
      "('female', 'male', 'bel')\n",
      "('female', 'male', 'bee')\n",
      "('female', 'male', 'bernie')\n",
      "('female', 'male', 'berty')\n",
      "('female', 'male', 'wallie')\n",
      "('female', 'male', 'zelda')\n",
      "('female', 'male', 'quentin')\n",
      "('female', 'male', 'quintilla')\n",
      "('female', 'male', 'kathi')\n",
      "('female', 'male', 'anne-corinne')\n",
      "('female', 'male', 'roseanne')\n",
      "('female', 'male', 'leigh')\n",
      "('female', 'male', 'onida')\n",
      "('female', 'male', 'nanon')\n",
      "('female', 'male', 'gussy')\n",
      "('female', 'male', 'katusha')\n",
      "('female', 'male', 'elsy')\n",
      "('female', 'male', 'yovonnda')\n",
      "('female', 'male', 'geeta')\n",
      "('female', 'male', 'myrtice')\n",
      "('female', 'male', 'denni')\n",
      "('female', 'male', 'analise')\n",
      "('female', 'male', 'devina')\n",
      "('female', 'male', 'vivyanne')\n",
      "('female', 'male', 'emmi')\n",
      "('female', 'male', 'ora')\n",
      "('female', 'male', 'penny')\n",
      "('female', 'male', 'birdie')\n",
      "('female', 'male', 'valeda')\n",
      "('female', 'male', 'constanta')\n",
      "('female', 'male', 'aleecia')\n",
      "('female', 'male', 'rachelle')\n",
      "('female', 'male', 'julie')\n",
      "('female', 'male', 'tracy')\n",
      "('female', 'male', 'jacquelynn')\n",
      "('female', 'male', 'roselia')\n",
      "('female', 'male', 'celestia')\n",
      "('female', 'male', 'martita')\n",
      "('female', 'male', 'aurilia')\n",
      "('female', 'male', 'winne')\n",
      "('female', 'male', 'myrtle')\n",
      "('female', 'male', 'rochella')\n",
      "('female', 'male', 'theresita')\n",
      "('female', 'male', 'renelle')\n",
      "('female', 'male', 'emylee')\n",
      "('female', 'male', 'selestina')\n",
      "('female', 'male', 'roseanna')\n",
      "('female', 'male', 'basia')\n",
      "('female', 'male', 'alta')\n",
      "('female', 'male', 'pegeen')\n",
      "('female', 'male', 'ottilie')\n",
      "('female', 'male', 'anestassia')\n",
      "('female', 'male', 'jacki')\n",
      "('female', 'male', 'pearle')\n",
      "('female', 'male', 'brear')\n",
      "('female', 'male', 'kary')\n",
      "('female', 'male', 'natalya')\n",
      "('female', 'male', 'melodee')\n",
      "('female', 'male', 'ninon')\n",
      "('female', 'male', 'idalia')\n",
      "('female', 'male', 'coreen')\n",
      "('female', 'male', 'virginia')\n",
      "('female', 'male', 'crissie')\n",
      "('female', 'male', 'christie')\n",
      "('female', 'male', 'cori')\n",
      "('female', 'male', 'francoise')\n",
      "('female', 'male', 'karoline')\n",
      "('female', 'male', 'ernaline')\n",
      "('female', 'male', 'sherilyn')\n",
      "('female', 'male', 'heloise')\n",
      "('female', 'male', 'kelli')\n",
      "('female', 'male', 'shane')\n",
      "('female', 'male', 'christyna')\n",
      "('female', 'male', 'junina')\n",
      "('female', 'male', 'korney')\n",
      "('female', 'male', 'arabel')\n",
      "('female', 'male', 'simona')\n",
      "('female', 'male', 'fiann')\n",
      "('female', 'male', 'laetitia')\n",
      "('female', 'male', 'cicely')\n",
      "('female', 'male', 'corey')\n",
      "('female', 'male', 'dorcas')\n",
      "('female', 'male', 'arlette')\n",
      "('female', 'male', 'corly')\n",
      "('female', 'male', 'tessy')\n",
      "('female', 'male', 'maud')\n",
      "('female', 'male', 'lamb')\n",
      "('female', 'male', 'cherish')\n",
      "('female', 'male', 'darsie')\n",
      "('female', 'male', 'harlie')\n",
      "('female', 'male', 'rosalinde')\n",
      "('female', 'male', 'felita')\n",
      "('female', 'male', 'leila')\n",
      "('female', 'male', 'carlota')\n",
      "('female', 'male', 'margery')\n",
      "('female', 'male', 'zuzana')\n",
      "('female', 'male', 'lucine')\n",
      "('female', 'male', 'maurine')\n",
      "('female', 'male', 'gennifer')\n",
      "('female', 'male', 'mollie')\n",
      "('female', 'male', 'maren')\n",
      "('female', 'male', 'heidie')\n",
      "('female', 'male', 'lanna')\n",
      "('female', 'male', 'nixie')\n",
      "('female', 'male', 'ilyse')\n",
      "('female', 'male', 'myrtia')\n",
      "('female', 'male', 'augustina')\n",
      "('female', 'male', 'tatiana')\n",
      "('female', 'male', 'trix')\n",
      "('female', 'male', 'christan')\n",
      "('female', 'male', 'samara')\n",
      "('female', 'male', 'christabel')\n",
      "('female', 'male', 'eddie')\n",
      "('female', 'male', 'rasla')\n",
      "('female', 'male', 'eileen')\n",
      "('female', 'male', 'rhianon')\n",
      "('female', 'male', 'diana')\n",
      "('female', 'male', 'corine')\n",
      "('female', 'male', 'sandra')\n",
      "('female', 'male', 'shawnee')\n",
      "('female', 'male', 'lisabeth')\n",
      "('female', 'male', 'cabrina')\n",
      "('female', 'male', 'julita')\n",
      "('female', 'male', 'daloris')\n",
      "('female', 'male', 'allis')\n",
      "('female', 'male', 'remy')\n",
      "('female', 'male', 'phil')\n",
      "('female', 'male', 'lib')\n",
      "('female', 'male', 'jonis')\n",
      "('female', 'male', 'fulvia')\n",
      "('female', 'male', 'britaney')\n",
      "('female', 'male', 'adey')\n",
      "('female', 'male', 'madlen')\n",
      "('female', 'male', 'bell')\n",
      "('female', 'male', 'alecia')\n",
      "('female', 'male', 'marlane')\n",
      "('female', 'male', 'angel')\n",
      "('female', 'male', 'barbey')\n",
      "('female', 'male', 'linette')\n",
      "('female', 'male', 'rafaelita')\n",
      "('female', 'male', 'meggie')\n",
      "('female', 'male', 'orelle')\n",
      "('female', 'male', 'mariette')\n",
      "('female', 'male', 'magna')\n",
      "('female', 'male', 'lavina')\n",
      "('female', 'male', 'sonnie')\n",
      "('female', 'male', 'lora')\n",
      "('female', 'male', 'donna')\n",
      "('female', 'male', 'karlyn')\n",
      "('female', 'male', 'marj')\n",
      "('female', 'male', 'tasha')\n",
      "('female', 'male', 'dottie')\n",
      "('female', 'male', 'zabrina')\n",
      "('female', 'male', 'allissa')\n",
      "('female', 'male', 'eugenie')\n",
      "('female', 'male', 'colly')\n",
      "('female', 'male', 'marja')\n",
      "('female', 'male', 'lee')\n",
      "('female', 'male', 'sallee')\n",
      "('female', 'male', 'daniela')\n",
      "('female', 'male', 'ammamaria')\n",
      "('female', 'male', 'fayina')\n",
      "('female', 'male', 'arleen')\n",
      "('female', 'male', 'julienne')\n",
      "('female', 'male', 'harley')\n",
      "('female', 'male', 'gertie')\n",
      "('female', 'male', 'nell')\n",
      "('female', 'male', 'noelle')\n",
      "('female', 'male', 'shana')\n",
      "('female', 'male', 'kimmie')\n",
      "('female', 'male', 'antonella')\n",
      "('female', 'male', 'ciel')\n",
      "('female', 'male', 'odelle')\n",
      "('female', 'male', 'daphne')\n",
      "('female', 'male', 'darcie')\n",
      "('female', 'male', 'sherye')\n",
      "('female', 'male', 'olympe')\n",
      "('female', 'male', 'bettina')\n",
      "('female', 'male', 'lynette')\n",
      "('female', 'male', 'prunella')\n",
      "('female', 'male', 'lynna')\n",
      "('female', 'male', 'paulita')\n",
      "('female', 'male', 'merola')\n",
      "('female', 'male', 'tammy')\n",
      "('female', 'male', 'theadora')\n",
      "('female', 'male', 'jacqui')\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    #print(name)\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )\n",
    "\n",
    "print('actual, guess, name: \\n')\n",
    "for x in errors:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our Max Entropy classifier uses an iterative method to maximize the performance of the training corpus classification. In this case the default number of iteration was 100, which is reasonable for our dataset. This is why it takes a long time to train a huge dataset and could also explain why it is not as popular. Another drawback with this classifer is that can only answer the questions about the conditional probabilities of the feature and label compared to the naive bayes.\n",
    "\n",
    "However, this classifier gives us options to associate more than one feature with a given label and vice versa. Another advantage is that the classifier does not require a vast amount of data like decision trees as long as the dataset has a high entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "http://www.nltk.org/howto/corpus.html\n",
    "\n",
    "[Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit](https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495/ref=mt_paperback?_encoding=UTF8&me=&qid=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
